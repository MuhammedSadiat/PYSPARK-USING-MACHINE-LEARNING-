{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee6d39a-0f6c-429e-b91f-3d7912ae5748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/21 14:38:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/21 14:38:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Classification Model with PySpark\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f681521-c0df-41cb-a9c8-c3d4573b1579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.10/site-packages (3.4.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/21 14:38:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/04/21 14:38:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/04/21 14:38:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/04/21 14:38:50 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "23/04/21 14:38:59 WARN DAGScheduler: Broadcasting large task binary with size 1033.7 KiB\n",
      "23/04/21 14:39:00 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "23/04/21 14:39:02 WARN MemoryStore: Not enough space to cache rdd_45_0 in memory! (computed 419.2 MiB so far)\n",
      "23/04/21 14:39:02 WARN BlockManager: Persisting block rdd_45_0 to disk instead.\n",
      "23/04/21 14:39:04 WARN MemoryStore: Not enough space to cache rdd_45_0 in memory! (computed 419.2 MiB so far)\n",
      "23/04/21 14:39:06 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "23/04/21 14:39:07 WARN MemoryStore: Not enough space to cache rdd_45_0 in memory! (computed 419.2 MiB so far)\n",
      "23/04/21 14:39:09 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "23/04/21 14:39:10 WARN MemoryStore: Not enough space to cache rdd_45_0 in memory! (computed 419.2 MiB so far)\n",
      "23/04/21 14:39:12 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "23/04/21 14:39:13 WARN MemoryStore: Not enough space to cache rdd_45_0 in memory! (computed 419.2 MiB so far)\n",
      "23/04/21 14:39:15 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.4%\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "\n",
    "# Importing libraries and modules\n",
    "import os\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Configure logging ERROR level\n",
    "logger = logging.getLogger(\"py4j\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Loading the dataset into a PySpark dataframe\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"false\").option(\"delimiter\", \"\\t\").load(\"amazon_cells_labelled.txt\")\n",
    "\n",
    "# Renaming the columns and casting the labels to integer type\n",
    "df = df.toDF(\"text\", \"label\")\n",
    "df = df.withColumn(\"label\", col(\"label\").cast(\"int\"))\n",
    "\n",
    "# Tokenizing the text data\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# Removing stop words from the text data\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"wordsfiltred\")\n",
    "df = stopwords_remover.transform(df)\n",
    "\n",
    "# Compute the term frequency of the filtered words\n",
    "hashing_tf = HashingTF(inputCol=\"wordsfiltred\", outputCol=\"rawfeatures\")\n",
    "df = hashing_tf.transform(df)\n",
    "\n",
    "# Computing the inverse document frequency of the raw features\n",
    "idf = IDF(inputCol=\"rawfeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df)\n",
    "df = idf_model.transform(df)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "(training_data, testing_data) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Training a random forest classifier\n",
    "random_forest = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100, maxDepth=4)\n",
    "rforest_model = random_forest.fit(training_data)\n",
    "\n",
    "# Evaluating the performance of the random forest classifier on the testing data\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(rforest_model.transform(testing_data))\n",
    "print(\"Accuracy: {:.1f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3538b6-c519-4bcf-8d9c-174673c14774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
